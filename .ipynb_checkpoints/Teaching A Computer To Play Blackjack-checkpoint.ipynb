{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaching A Computer To Play Blackjack\n",
    "\n",
    "*Authors:*\n",
    "- Kelsey Cribari - Kelsey.Cribari@gmail.com\n",
    "- Brandt Reutimann - brandtreutimann@gmail.com\n",
    "- Courtney Schulze - Courtney.Schulze@colostate.edu\n",
    "\n",
    "*Date: December, 2017*\n",
    "\n",
    "*GitHub Repository: https://github.com/BrandtRobert/BlackJackAI *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Overview](#overview)\n",
    "* [Method](#method)\n",
    "    * [Phase One](#phaseone)\n",
    "         * [Deck](#deck)\n",
    "         * [Player](#player)\n",
    "         * [Dealer](#dealer)\n",
    "    * [Phase Two](#phasetwo)\n",
    "        *  [Game](#game)\n",
    "        *  [Reinforcement Learning](#reinlearn)\n",
    "        *  [Exploring Learning Rate & Epsilon Decay](#exploring)\n",
    "        *  [Betting Strategy](#betstrat)\n",
    "* [Results](#results)\n",
    "   * [Comparing our agent to a random player](#crandom)\n",
    "   * [Comparing our agent to \"proper\" blackjack strategy](#cproper)\n",
    "   * [Betting strategy & earnings](#earnings)\n",
    "* [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a id=\"intro\"></a>\n",
    " \n",
    "*The What. The Why. The How.*\n",
    "\n",
    "It all started when a member of the iconic trio went to Las Vegas for a weekend over the semester. While she had never gambled before, everyone would soon find out that Courtney loved blackjack ... a lot. Although her wallet was empty, Courtney left Vegas rich in experience and Blackjack knowledge. Born out of that trip and lost money came an idea, an idea for a CS440 final project.\n",
    "\n",
    "For those of you reading this report who might not know, in the game of blackjack, players are dealt two cards, and they are dealt cards until they reach or get as close as then can to a card value totaling twenty-one. If the player’s total goes over twenty-one, the player busts and they lose their money. If the player’s total is less than the dealer’s total, the player also loses their money. Really, the player tends to lose their money quite a lot. However, there is a generally universally accepted basic blackjack strategy. If players play with this proper strategy, then the dealer only has about a 0.05% advantage. In other words, “if you are playing for $100 per hand, you can expect to lose about 50 cents each hand.” A chart of this proper strategy is found below:\n",
    "\n",
    "![alt text](https://www.blackjackclassroom.com/wp-content/uploads/2017/02/Blackjack-Basic-Strategy-Chart.png \"Basic Blackjack Strategy\")\n",
    "(Photo courtesy of [Blackjack Classroom](https://www.blackjackclassroom.com/blackjack-basic-strategy-charts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this interesting nature of the basic blackjack strategy, we thought it would be a fascinating project to try training a computer to play blackjack through basic artifical intelligence principles, namely reinforcement learning. Would the computer naturally learn the strategy that all professional blackjack players accept as “proper”? That’s what we set to find out.\n",
    "<a id=\"overview\"></a>\n",
    "### Overview of Methods:\n",
    "* Temporal Difference Reinforcement Learning: In the inital implementation of the game, we used reinforcements of 1 if you beat the dealer, 0 if you push (tie with the dealer), and -1 if you lose. (These reinforcements change if doubling down is implemented.)\n",
    "\n",
    "### Overview of Results:\n",
    "* With reinforcement learning implemented, the win rate for our agent became about 45%, which is on par if someone is using proper blackjack strategy.\n",
    "* Additionally, when comparing the Q table with the proper blackjack chart above, the moves the player makes using the Q table matches about 77% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method <a id=\"method\"></a>\n",
    "\n",
    "*The steps we took. The resources we used. The work we shared.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase One:<a id=\"phaseone\"></a>\n",
    "\n",
    "The first phase was comprised of creating deck, player, and dealer representations for the game. Feel free to run the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deck:<a id=\"deck\"></a>\n",
    "\n",
    "*Main Author: Brandt Reutimann*\n",
    "\n",
    "The deck is a relatively standard card deck, except suits are naturally ignored. All face cards are represented by 10's (since for blackjack, a king is the same as a queen which is the same as a ten). \n",
    "\n",
    "A new deck is shuffled by default. This feature can be turned off by passing ```deck = BlackJackDeck (shuffleCards = False)```.\n",
    "\n",
    "Suits and faces can be activated with ```deck = BlackJackDeck (SuitsAndFaces = True)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.Deck import BlackJackDeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Example of drawing 10 random cards without suits or faces:\")\n",
    "deck = BlackJackDeck()\n",
    "for i in range (0, 10):\n",
    "    print (deck.drawCard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Example of drawing 10 random cards with suits and faces:\")\n",
    "deckSuits = BlackJackDeck(SuitsAndFaces = True)\n",
    "for i in range (0, 10):\n",
    "    print (deckSuits.drawCard())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player:<a id=\"player\"></a>\n",
    "\n",
    "*Main Author: Courtney Schulze*\n",
    "\n",
    "A player consists of a hand and a current card count (which is the current card total of the cards in their hand). A player has a list of valid moves they can make (either stand or hit). When a player hits, a card is drawn from the deck and added to the player's hand. If the card count after the new card is greater than 21, then the player busts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.Player import Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create both deck and player\n",
    "deck = BlackJackDeck()\n",
    "player = Player()\n",
    "\n",
    "#put first two cards in player's hand\n",
    "player.addCardToHand(deck.drawCard())\n",
    "player.addCardToHand(deck.drawCard())\n",
    "\n",
    "print(\"The player's current hand is: \" + str(player.getHand()))\n",
    "print(\"The cards in the player's hand total: \" + str(player.getCardCount()))\n",
    "print(\"Valid moves: \" + str(player.validMoves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The player takes a card. Here are the results: \" + str(player.hit(deck)))\n",
    "print(\"The player busted based on the previous hit: \" + str(player.bust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the player keeps hitting until they get to a total greater than 21?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (player.getCardCount() < 21):\n",
    "    while not player.bust:\n",
    "        result = player.hit(deck)\n",
    "        print(\"Player's hand: \" + str(player.getHand()))\n",
    "    print(\"Player busted! Card total was: \" + str(player.getCardCount()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealer:<a id=\"dealer\"></a>\n",
    "\n",
    "*Main Author: Kelsey Cribari*\n",
    "\n",
    "A dealer is pretty much a player, but they have a more specific ruleset they have to follow in terms of valid moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.Dealer import BlackJackDealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create both dealer and deck\n",
    "deck = BlackJackDeck()\n",
    "dealer = BlackJackDealer()\n",
    "\n",
    "dealer.hand.append(deck.drawCard())\n",
    "# deal second card to dealer\n",
    "dealerFaceCard = deck.drawCard()\n",
    "dealer.hand.append(dealerFaceCard)\n",
    "# keep track of the card that is face up on the dealer so the player knows what to base their moves off of\n",
    "dealer.faceUpCard = dealerFaceCard\n",
    "\n",
    "print(\"Dealer's faceup card: \" + str(dealer.faceUpCard))\n",
    "print(\"Dealer's hand: \" + str(dealer.hand))\n",
    "print(\"The dealer has to: \" + str(dealer.dealerValidMoves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Two:<a id=\"phasetwo\"></a>\n",
    "\n",
    "This phase consisted of doing the game representation and writing the trainQ and testQ methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game<a id=\"game\"></a>\n",
    "\n",
    "*Main Authors: Kelsey Cribari and Brandt Reutimann*\n",
    "\n",
    "A game has a deck, a dealer and a player. The game's deal function is called to initate a game. The deal function deals a card to the player then a card to the dealer, then a second card to the player, then a second card to the dealer. If the currect deck is empty, it generates a new random deck. \n",
    "\n",
    "The game also consists of the trainQ and testQ functions which are explained below as well as a few helper functions. \n",
    "\n",
    "A full game includes the training of a player on a given number of rounds. Then the player is tested against the dealer to determine a win percentage. The goal is to get a player to make a statistically correct play (or a play that maximizes the player's chance of winning) given the card the dealer is showing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.Game import BlackJackGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look below to see how a game is played between a player and a dealer. In this scenario, the player's decision making is based on whether or not they are below 21. Of course this isn't the best strategy, but the goal is to show the sequence of events for one game. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = BlackJackGame()\n",
    "game.deal()\n",
    "print(\"Player hand \" + str(game.player.hand))\n",
    "print(\"Dealer hand \" + str(game.dealer.hand))\n",
    "print(\"Dealer face up card \" + str(game.dealer.faceUpCard))\n",
    "\n",
    "while game.player.cardCount <= 21:\n",
    "    newHand = game.player.hit(game.deck)\n",
    "\n",
    "print(\"Player's hand after hitting \" + str(newHand))\n",
    "\n",
    "result = game.dealer.playTurn(game.deck, game.player)\n",
    "\n",
    "print(\"Outcome for the dealer: \" + str(result))\n",
    "if result == 'win':\n",
    "    playerResult = 'loss'\n",
    "elif result == 'loss':\n",
    "    playerResult = 'win'\n",
    "else:\n",
    "    playerResult = result\n",
    "print(\"Therefore, the result of the hand for the player is: \" + str(playerResult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning <a id=\"reinlearn\"></a>\n",
    "\n",
    "*Main Authors: Kelsey Cribari and Brandt Reutimann*\n",
    "\n",
    "We used a temporal difference strategy for training the player agent in our BlackJack game. Essentially the player is reinforced +1 for a win, -1 for a loss, and 0 for a push (draw). If the player doubles down and wins, it is reinforced +2, but conversely if it loses it is reinforced -2. All the reinforcement is contained in our trainQ function.\n",
    "\n",
    "We then use a testQ function to simulate playing a number of games. The testQ function keeps track of wins and earnings to simulate how well the agent performs using it's knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = BlackJackGame()\n",
    "Q = game.trainQ(100000, .6, .8)\n",
    "print ('calling testQ')\n",
    "winRate, earnings = game.testQ(Q, 1000, verbose=True)\n",
    "print ('Win rate was: {}, Earnings: ${}'.format(winRate, earnings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Learning Rate & Epsilon Decay<a id=\"exploring\"></a>\n",
    "\n",
    "*Main Authors: Courtney Schulze*\n",
    "\n",
    "In order to make our learning algorithm effective, we wanted to find the right combination of learning rate and epsilon decay that would maximize winning. \n",
    "\n",
    "If learning rate is too high the agent might become too confident if it got lucky on a move that is actually bad.\n",
    "For example, let's say the agent hits on a 20 against a dealer 10 and luckily receives an Ace, thus winning the hand. If the learning rate is too high, the agent may have to much confidence in hitting in this scenario again. Or the opposite scenario is that AI becomes risk adverse as result of hitting and losing in a strategically correct scenario.\n",
    "\n",
    "For epsilon, we want to insure that the agent explores as many random moves as possible. BlackJack has a lot of possible scenarios for the agent to encounter. There are combinations of sums and Aces all the way up to 21, in addition to variance in the dealer's upcard. In order to learn to be a BlackJack expert, the agent has to encounter all of these scenarios and explore different moves to determine which strategy is best. If epsilon is too low, the AI might not enough experimentation before relying on it's own experience (Q table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averageWinRate(learningRate, epsilonDecayFactor):\n",
    "    sumWin = 0.0\n",
    "    for i in range(10):\n",
    "        game = BlackJackGame()\n",
    "        Q = game.trainQ(100000, learningRate, epsilonDecayFactor)\n",
    "        winRate, _ = game.testQ(Q, 1000)\n",
    "        sumWin += winRate\n",
    "    \n",
    "    return sumWin / 10\n",
    "\n",
    "print(\"Average win rate for learningRate = 0.6 and epsilonDecayFactor = 0.8: \" + str(averageWinRate(0.6, 0.8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most versions of the average win rate above, the win rate comes out to about 45%. This makes sense: when playing with proper strategy, the house should only have about a 0.05% advantage, bringing the player's win rate to 45%. According to [Wizard of Odds](https://wizardofodds.com/games/blackjack/appendix/4/), the probability of a net win in blackjack is 42.42%. If ties are ignored, that jumps to about 46.35%. However, we wondered if we could make that win rate better by playing around with the learning rate and epsilon decay factor for 100,000 iterations. Therefore, Courtney put on her investigation hat to see how high we could get that win rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testDifferentValues():\n",
    "    print(\"Testing learningRate = 0.5 and epsilonDecayFactor = 0.5\")\n",
    "    print(\"Average win rate for learningRate = 0.5 and epsilonDecayFactor = 0.5: \" + str(averageWinRate(0.5, 0.5)))\n",
    "    \n",
    "    print(\"Testing learningRate = 0.99 and epsilonDecayFactor = 0.3\")\n",
    "    print(\"Average win rate for learningRate = 0.99 and epsilonDecayFactor = 0.3: \" + str(averageWinRate(0.99, 0.3)))\n",
    "    \n",
    "    print(\"Testing learningRate = 0.99 and epsilonDecayFactor = 0.8\")\n",
    "    print(\"Average win rate for learningRate = 0.99 and epsilonDecayFactor = 0.8: \" + str(averageWinRate(0.99, 0.8)))\n",
    "    \n",
    "    print(\"Testing learningRate = 0.3 and epsilonDecayFactor = 0.3\")\n",
    "    print(\"Average win rate for learningRate = 0.3 and epsilonDecayFactor = 0.3: \" + str(averageWinRate(0.3, 0.3)))\n",
    "    \n",
    "    print(\"Testing learningRate = 0.3 and epsilonDecayFactor = 0.99\")\n",
    "    print(\"Average win rate for learningRate = 0.3 and epsilonDecayFactor = 0.99: \" + str(averageWinRate(0.3, 0.99)))\n",
    "\n",
    "print(\"Please be prepared to wait a while.\")\n",
    "testDifferentValues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at these results, the best thing to do seems to be use a bit lower of a learning rate, and don't make the epsilonDecayFactor too large or small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betting Strategy<a id=\"betstrat\"></a>\n",
    "\n",
    "In order to make the game more interesting we wanted to incorporate some betting strategy. We figured that implementing a learning strategy for betting would be trivial. This is because in the long run the goal is to maximize earnings. Therefore, the agent should eventually determine that it is losing money the longer it plays and hence always gamble the minimum. This is an assumption that will need to be tested in future experiments.\n",
    "\n",
    "For our project we decided to use a different approach to bettting. We call this strategy hotstreak betting. Essentially, the more wins the agent gets consecutively the higher it will bet with each consecutive win. We wanted to model it after the fallacy of many gamblers: \"I'm on a lucky streak right now, I should bet more\". We thought this was an interesting notion to test in our agent, especially for observing how this betting strategy effects earnings.\n",
    "\n",
    "We implemented this strategy by keeping a count of consecutive wins. If the agent loses this count is reset to 0 and if the agent draws we decrement the count by 1 (maybe shaking its feeling of luck a little). Then in a determineBet function we use a soft exponential curve to determine how much the agent will bet. The hardness (exponetial slope) of this curve can be modified by increasing the streak factor. You can also disable this hotstreak betting strategy by setting the hotstreak flag to false. How much the player bets is based on a minimum bet amount, if there is no hotstreak strategy then the agent always bets the minimum unless doubling down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement hotstreak betting\n",
    "def determineBet (self, consecutiveWins, minBet = 5, streakFactor = 1.2, hotstreak = True):\n",
    "    if hotstreak is False:\n",
    "        return minBet\n",
    "    return round(minBet ** streakFactor) if consecutiveWins > 0 else minBet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results<a id=\"results\"></a>\n",
    "\n",
    "*The results we got.*\n",
    "\n",
    "#### Comparing our agent to a random player<a id=\"crandom\"></a>\n",
    "\n",
    "As we have observed earlier, our agent's win rate will hover around 45 - 46%. Let's see how this compares to a random player.\n",
    "\n",
    "We are going to modify averageWinRate to take a few extra arguments. Mostly, we want to specify iterations to see if our agent improves with more training. Also, we want to add an argument to calculate average win rate for a random player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify average win rate to include iteration parameter\n",
    "def averageWinRate(learningRate, epsilonDecayFactor, iterations = 100000, random = False):\n",
    "    sumWin = 0.0\n",
    "    for i in range(10):\n",
    "        game = BlackJackGame()\n",
    "        Q = {}\n",
    "        if random:\n",
    "            winRate, _ = game.testQ(Q, 5000, esp = 1)\n",
    "        else:\n",
    "            Q = game.trainQ(iterations, learningRate, epsilonDecayFactor)\n",
    "            winRate, _ = game.testQ(Q, 5000) # Also up the number of games played to 5000\n",
    "        sumWin += winRate\n",
    "    \n",
    "    return sumWin / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winRates = []\n",
    "iters = []\n",
    "for i in range (1, 6):\n",
    "    # Test iterations in powers of 10\n",
    "    iterations = 10 ** i\n",
    "    iters.append(iterations)\n",
    "    st = time.time()\n",
    "    avgWin = averageWinRate(.3, .99, iterations = iterations)\n",
    "    et = time.time()\n",
    "    winRates.append(avgWin)\n",
    "    print ('Training complete for {} iterations. Time {:.2f} seconds'.format(iterations, et - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randRates = []\n",
    "rIters = []\n",
    "for i in range (1,6):\n",
    "    iterations = 10 ** i\n",
    "    rIters.append(iterations)\n",
    "    avgWin = averageWinRate(.3, .99, random = True)\n",
    "    randRates.append(avgWin)\n",
    "randRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(iters, winRates)\n",
    "plt.plot(rIters, randRates)\n",
    "plt.ylabel('Win rate (%)')\n",
    "plt.xlabel('Iterations')\n",
    "# plt.figure(figsize=(14,4), dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing our agent to \"proper\" blackjack strategy<a id=\"cproper\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a win rate of about 45% is fine and dandy, and having a higher win rate than a random player is great as well. However, our initial research question wanted to take a look at if by using reinforcement learning, a computer would learn \"proper\" blackjack strategy. The code below creates a Q table and constructs a basic chart from that Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = BlackJackGame()\n",
    "Q = game.trainQ(100000, .6, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestMoveGivenState(Q, playerVal, dealerUp, verbose = False):\n",
    "    reinforcement = float('-inf')\n",
    "    move = None\n",
    "    if Q.get((playerVal, dealerUp, 'hit'), float(\"-inf\")) > reinforcement:\n",
    "        reinforcement = Q[(playerVal, dealerUp, 'hit')]\n",
    "        move = 'H'\n",
    "    if Q.get((playerVal, dealerUp, 'stand'), float(\"-inf\")) > reinforcement:\n",
    "        reinforcement = Q[(playerVal, dealerUp, 'stand')]\n",
    "        move = 'S'\n",
    "    if Q.get((playerVal, dealerUp, 'double'), float(\"-inf\")) > reinforcement:\n",
    "        reinforcement = Q[(playerVal, dealerUp, 'double')]\n",
    "        move = 'D'\n",
    "    \n",
    "    if (verbose):\n",
    "        print(\"Reinforcement for HIT: \" + str(Q[(playerVal, dealerUp, 'hit')]))\n",
    "        print(\"Reinforcement for STAND: \" + str(Q[(playerVal, dealerUp, 'stand')]))\n",
    "        print(\"Reinforcement for DOUBLE: \" + str(Q[(playerVal, dealerUp, 'double')]))\n",
    "        \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestMoveGivenState(Q, 19, 'A', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def percentageMatching(Q):\n",
    "    matching = 0\n",
    "    dealerUp = [2, 3, 4, 5, 6, 7, 8, 9, 10, 'A']\n",
    "    \n",
    "    #here are the rows in the actual blackjack chart\n",
    "    twentyone = ['S']*10\n",
    "    sixteen = ['S']*5 + ['H']*5\n",
    "    twelve = ['H']*2 + ['S']*3 + ['H']*5\n",
    "    eleven = ['D']*9 + ['H']\n",
    "    ten = ['D']*8 + ['H']*2\n",
    "    nine = ['H'] + ['D']*4 + ['H']*5\n",
    "    eight = ['H']*10\n",
    "    \n",
    "    #calculate 17+\n",
    "    counter = 0\n",
    "    for up in dealerUp:\n",
    "        if bestMoveGivenState(Q, 21, up) is twentyone[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 20, up) is twentyone[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 19, up) is twentyone[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 18, up) is twentyone[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 17, up) is twentyone[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 16, up) is sixteen[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 15, up) is sixteen[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 14, up) is sixteen[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 13, up) is sixteen[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 12, up) is twelve[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 11, up) is eleven[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 10, up) is ten[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 9, up) is nine[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 8, up) is eight[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 7, up) is eight[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 6, up) is eight[counter]:\n",
    "            matching += 1\n",
    "        if bestMoveGivenState(Q, 5, up) is eight[counter]:\n",
    "            matching += 1\n",
    "        counter += 1\n",
    " \n",
    "    return (matching / 170) * 100\n",
    "\n",
    "game = BlackJackGame()\n",
    "Q = game.trainQ(100000, .6, .8)\n",
    "percentageMatching(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount our Q table matches the top-third of the proper blackjack chart is usually around 77% ... not bad! This method doesn't account for if there's an ace in the player's hand. The agent doesn't seem to like doubling down a lot, and I don't blame it! Better to play it safe.\n",
    "\n",
    "Let's take a look at how much the Q table matches the chart if we train it for a different number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 1\n",
    "iters = []\n",
    "perCorrect = []\n",
    "for i in range(6):\n",
    "    iters.append(iterations)\n",
    "    game = BlackJackGame()\n",
    "    Q = game.trainQ(iterations, 0.6, 0.8)\n",
    "    perCorrect.append(percentageMatching(Q))\n",
    "    print ('Training complete for {} iterations.'.format(iterations))\n",
    "    iterations = iterations * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iters, perCorrect)\n",
    "plt.ylabel('Percentage Matching \"Proper\" Blackjack Strategy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betting Strategy and Earnings <a id=\"earnings\"></a>\n",
    "\n",
    "Let's observe how well our players do with hotstreak betting. We will observe earnings for an increasing amount of games played for our agent and a random player. We want to compare how they do when they hotstreak bet, against how they do when they don't hotstreak bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averageEarnings(games, random = False, hotstreak = False):\n",
    "    sumEarned = 0.0\n",
    "    for i in range(10):\n",
    "        game = BlackJackGame()\n",
    "        Q = {}\n",
    "        if random:\n",
    "            winRate, earnings = game.testQ(Q, games, esp = 1, verbose = False, hotstreak = hotstreak)\n",
    "        else:\n",
    "            Q = game.trainQ(25000, .5, .99)\n",
    "            winRate, earnings = game.testQ(Q, games, verbose = False, hotstreak = hotstreak)\n",
    "        sumEarned += earnings\n",
    "    \n",
    "    return sumEarned / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def earningResults (random, hotstreak):\n",
    "    earned = []\n",
    "    games = []\n",
    "    for i in range (1, 6):\n",
    "        # Test iterations in powers of 10\n",
    "        gs = 1000 * i\n",
    "        games.append(gs)\n",
    "        st = time.time()\n",
    "        avgEarn = averageEarnings(gs, random = random, hotstreak = hotstreak)\n",
    "        et = time.time()\n",
    "        earned.append(avgEarn)\n",
    "        print ('Training complete for {} iterations. Time {:.2f} seconds'.format(gs, et - st))\n",
    "    return earned, games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Average training time is ~ 3 minutes')\n",
    "games = [1000, 2000, 3000, 4000, 5000]\n",
    "print (\"No random, No hotstreak\")\n",
    "agentEarned, _ = earningResults (False, False)\n",
    "print (\"No random, Yes hotstreak\")\n",
    "agentHot, _ = earningResults (False, True)\n",
    "print (\"Yes random, No hotstreak\")\n",
    "randEarned, _ = earningResults (True, False)\n",
    "print (\"Yes random, Yes hotstreak\")\n",
    "randHot, _ = earningResults (True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(games, agentEarned, label = \"Non-random, no hotstreak\")\n",
    "plt.plot(games, agentHot, label = \"Non-random, with hotstreak\")\n",
    "plt.title('Non-random Player')\n",
    "plt.ylabel('Earnings')\n",
    "plt.xlabel('Games Played')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(games, randEarned, label = \"Random, no hotstreak\")\n",
    "plt.plot(games, randHot, label = \"Random, with hotstreak\")\n",
    "plt.title('Random Player')\n",
    "plt.ylabel('Earnings')\n",
    "plt.xlabel('Games Played')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the charts above we can make a few observations.\n",
    "1. One obvious observation is that our agent loses substantially less money than a random player. The random can lose up to 10x as much as our trained agent.\n",
    "2. Another observation is that for both random and non-random players we can see that when using a hotstreak strategy they actually lose more in the long run. Although the difference may not seem substantial at a glance remember that a minimum bet is 5. This means that a difference of a 100 is that of 20 lost games in a row. Ouch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions<a id=\"conclusions\"></a>\n",
    "*The things we learned.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Phew, that was a lot of information. So what does it all really mean? Did we actually teach a computer to play blackjack? Yes! Did we find a way to give a player a 100% win percentage and never lose against the dealer? Unfortunately, no. But of course not. In the end, blackjack is a game of chance that is never in the player's favor (or else the casinos wouldn't make any money!). The goal was to see if we could train a computer to make \"proper\" plays to maximize win percentage. And here's a bit of what we found: \n",
    "\n",
    "We found out that our trained agent does a pretty good job of playing blackjack. When comparing the trained agent with a random player, we concluded that even a little bit of strategy, even if it isn't the \"proper\" strategy, will dramatically increase your chances of making money.\n",
    "\n",
    "Our trained agent's win percentage was actually comparable to maximum win strategy obtained by using the \"proper\" plays. However, this result is interesting for a couple of reasons: \n",
    "\n",
    "First, in blackjack at a casino, a player would also normally have the option to \"split\" their hand. What this means is that if the two cards they are dealt are the same, their inital bet could be applied to the first card and they can place a second bet on the second card and they would essentially split their one hand into two new hands. While sometimes this can be a poor decision, there are times where this would potentially yield a higher win percentage. For example, if you were dealt two 8's, your total card count would be 16 which is not such a great hand. But if you split it into two 8's, your chances of winning the both the hands are far better than winning one hand with a 16. The same situation applies for 2 Aces. \n",
    "\n",
    "Second, when we tested our Q table against the \"proper\" strategy table, we found out that the decision of our trained agent aligned with the table about 77% of the time. While that is pretty good, it is not 100%. What this means is that about 23% of the time, our agent is making a different decision, but our win percentage is still comparable with the \"proper\" win percentage. Hmmm... \n",
    "\n",
    "So how I ask you? HOW? \n",
    "\n",
    "Well the answer to that question is a little unclear. It looks like the computer makes some different decisions that seem to pan out as well as the \"proper\" strategy. What this might tell us is that there is more than one correct decision that will lead to a win. This is an interesting finding and one that would need to be explored a little further. But it makes you wonder if there is a way to train an agent to increase the win percentage to exceed the proper strategy. \n",
    "\n",
    "So what about lucky streaks? Picture this scenario. You sit down at a blackjack table in Vegas and you start to play. And you start to win. Hand after hand it looks like you can't be stopped. You've got a bunch of chips in front of you and you're starting to get confident. So you think, it won't hurt to double my bet on this hand, I can't lose! We would call this a \"hotstreak\" and we tested to see if that would increase your returns. However, with a little bit of \"lucky streak\" betting, we concluded that a player will actually lose more money in the long run if they bet based on lucky streaks. Bummer!\n",
    "\n",
    "\n",
    "This project was interesting and we had a lot of fun investigating betting strategy in blackjack. We learned that reinforcement learning, when tweaked with the right balance of learning rate and epsilon, and when trained on a high enough number of iterations, could be used to teach a computer not just to play blackjack but to do so pretty well. We also learned that as much as we want to take our grocery money and turn it into riches, there is no way to beat the system in a game of chance. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
